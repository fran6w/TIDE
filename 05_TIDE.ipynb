{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conférences Python Master TIDE #5\n",
    "\n",
    "1. **Web scraping**\n",
    "2. **API**\n",
    "\n",
    "&copy; 2025 Francis Wolinski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Rappels\n",
    "\n",
    "1. **Protocoles HTTP / HTTPS**\n",
    "\n",
    "- **HTTP** : HyperText Transfer Protocol, protocole permettant au client (navigateur / script) de communiquer avec un serveur web.\n",
    "- **HTTPS** : HyperText Transfer Protocol Secure, version sécurisée de HTTP, chiffrée via TLS (Transport Layer Security).\n",
    "\n",
    "Il existe plusieurs méthodes dont :\n",
    "- *GET* : envoie une requête pour obtenir une ressource. Les paramètres sont dans l’URL.\n",
    "- *POST* : envoie des données dans le corps de la requête (formulaires, connexions), plus adapté pour transmettre des informations sensibles ou volumineuses.\n",
    "\n",
    "Les **entêtes HTTP** (HTTP headers) sont des informations supplémentaires envoyées avec chaque requête et réponse HTTP. Elles décrivent le contexte de la communication : type de contenu attendu ou envoyé (`Content-Type`), informations sur le client (`User-Agent`), gestion du cache (`Cache-Control`), cookies, authentification, encodage, etc. Pour le web scraping, modifier certains entêtes — par exemple le `User-Agent` — permet de se faire reconnaître comme un navigateur classique, tandis qu’en lire d’autres aide à comprendre comment le serveur gère la session ou les données retournées.\n",
    "\n",
    "2. **HTML et balises de base**\n",
    "\n",
    "**HTML** : HyperText Markup Language, langage qui structure le contenu d’une page web :\n",
    "\n",
    "- `<p>…</p>` : paragraphe de texte.\n",
    "- `<br />` : retour à la ligne (balise autofermante).\n",
    "\n",
    "Ces balises constituent la structure que le web scraper va analyser pour extraire des données.\n",
    "\n",
    "3. **CSS**\n",
    "\n",
    "**CSS** : Cascading Style Sheets\n",
    "\n",
    "- *id* : identifiant unique d’un élément dans une page, utile pour cibler précisément un élément,\n",
    "- *class* : groupe d’éléments partageant le même style ou comportement),\n",
    "- *style* : définition directe d’un style sur l’élément.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "```html\n",
    "<p id=\"1234\", class=\"class1 class2\", style=\"color:blue;font-size:20px;\">Exemple de paragraphe.</p>\n",
    "```\n",
    "\n",
    "En scraping, les tags et les attributs *id* et *class* servent souvent à sélectionner les bons éléments dans le DOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"1234\", class=\"class1 class2\", style=\"color:blue;font-size:20px;\">Exemple de paragraphe.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping\n",
    "\n",
    "- Extraction d'informations d'un site web.\n",
    "- A utiliser en l'absence de données ouvertes ou d'API.\n",
    "- Technique fragile car le site web peut changer du jour au lendemain.\n",
    "- Problématique juridique...\n",
    "\n",
    "**Avec requests et Beautiful Soup**\n",
    "\n",
    "Doc :\n",
    "- **requests** : https://requests.readthedocs.io/en/master/\n",
    "- **Beautiful Soup** : https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "\n",
    "Pages de test :\n",
    "- https://yotta-conseil.fr/cours/page2.html\n",
    "- https://yotta-conseil.fr/cours/page3.html\n",
    "\n",
    "Miroir du site : http://kim.fspot.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://yotta-conseil.fr/cours/page2.html')\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type\n",
    "type(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str en précisant un encodage\n",
    "content = r.content.decode('utf-8')\n",
    "type(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accès au texte brut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accès au texte brut\n",
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour extraire des éléments particuliers, on utilise différentes méthodes :\n",
    "    \n",
    "- `.find(tag, attrs)` : trouve le premier tag avec les attributs spécifiés (en général *id* et/ou *class*)\n",
    "- `.findAll(tag, attrs)` : trouve tous les tags avec les attributs spécifiés (en général *id* et/ou *class*)\n",
    "- `.select(css)` : trouve tous les tags avec les CSS spécifiées\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La technique consiste par exemple à alimenter un dictionnaires avec les valeurs trouvées :\n",
    "- soit en utilisant `tag.attrs['attribut']` pour collecter la valeur `'attribut'` du tag\n",
    "- soit en utilisant `tag.text` pour collecter la valeur `text` située entre les balises ouvrante et fermante,\n",
    "- on peut aussi utiliser d'autres méthodes comme `.select()` qui joue sur la CSS ou `.next_sibling()` pour collecter le tag identique suivant,\n",
    "- éventuellement en recherchant dans un nouveau tag à l'intérieur d'un tag donné\n",
    "- parfois, il faut tester si la méthode `.find()` retourne bien un élément ou `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration du HTML\n",
    "\n",
    "div_tag = soup.find('div', attrs={'class': 'pricing-table'})\n",
    "div_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode :\n",
    " 1. On collecte les informations d'un  tag pour un seul enregistrement sous la forme d'un dictionaire.\n",
    " 2. On généralise en bouclant sur tous les tags de même nature et on rassemble les dictionnaires dans une liste.\n",
    " 3. On transforme la liste de dictionaires en DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. collecte des informations pour un enregistrement\n",
    "row = {}\n",
    "\n",
    "div_tag = soup.find('div', attrs={'class': 'pricing-table'})\n",
    "\n",
    "# type : Personal, Small Business, Enterprise\n",
    "h2_tag = div_tag.find(\"h2\")\n",
    "row[\"type\"] = h2_tag.text\n",
    "\n",
    "# price\n",
    "span_tag = div_tag.find(\"span\", attrs={'class': 'pricing-table-price'})\n",
    "row[\"price\"] = span_tag.text.strip().split()[0]\n",
    "\n",
    "# storage, database\n",
    "lis = div_tag.select(\"li\")\n",
    "row[\"storage\"] = lis[3].text.split()[0]\n",
    "row[\"database\"] = lis[4].text.split()[0]\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. collecte des informations pour tous les enregistrements\n",
    "rows = []\n",
    "\n",
    "for div_tag in soup.find_all('div', attrs={'class': 'pricing-table'}):\n",
    "    row = {}\n",
    "    \n",
    "    # type : Personal, Small Business, Enterprise\n",
    "    h2_tag = div_tag.find(\"h2\")\n",
    "    row[\"type\"] = h2_tag.text\n",
    "    \n",
    "    # price\n",
    "    span_tag = div_tag.find(\"span\", attrs={'class': 'pricing-table-price'})\n",
    "    row[\"price\"] = span_tag.text.strip().split()[0]\n",
    "    \n",
    "    # storage, database\n",
    "    lis = div_tag.select(\"li\")\n",
    "    row[\"storage\"] = lis[3].text.split()[0]\n",
    "    row[\"database\"] = lis[4].text.split()[0]\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. transformation en DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec une fonction\n",
    "def get_product_info(div_tag):\n",
    "    row = {}\n",
    "\n",
    "    # type : Personal, Small Business, Enterprise\n",
    "    h2_tag = div_tag.find(\"h2\")\n",
    "    row[\"type\"] = h2_tag.text\n",
    "\n",
    "    # price\n",
    "    span_tag = div_tag.find(\"span\", attrs={'class': 'pricing-table-price'})\n",
    "    row[\"price\"] = span_tag.text.strip().split()[0]\n",
    "\n",
    "    # storage, database\n",
    "    lis = div_tag.select(\"li\")\n",
    "    row[\"storage\"] = lis[3].text.split()[0]\n",
    "    row[\"database\"] = lis[4].text.split()[0]\n",
    "    \n",
    "    return row\n",
    "\n",
    "def extract_products(url):\n",
    "    try:\n",
    "        r = requests.get('https://yotta-conseil.fr/cours/page2.html')\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # Soup\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    # list comprehension with all results\n",
    "    rows = [get_product_info(div_tag) for div_tag in soup.find_all('div', attrs={'class': 'pricing-table'})]\n",
    "\n",
    "    # DataFrame with all results\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = extract_products(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercices**\n",
    "\n",
    "- Remplacer la méthode `.find_all()` par la méthode `.select()`\n",
    "- Web scrapping de la page : https://yotta-conseil.fr/cours/page3.html\n",
    "- Web scrapping de la page : https://books.toscrape.com/catalogue/category/books/travel_2/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inconvénients du web scraping:\n",
    "- plutôt lent (car on parse potentiellement beaucoup de HTML inutile)\n",
    "- ne donne pas les résultats attendus si une partie du contenu est intégré dynamiquement à la page via javascript\n",
    "- un changement dans l'architecture du html ou du css (e.g: refonte du design du site) oblige à réécrire entièrement le programme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API\n",
    "\n",
    "Exemple : Deezer\n",
    "\n",
    "Artiste : https://www.deezer.com/fr/artist/3037\n",
    "\n",
    "Récupérer le nombre de fans d'un artiste avec requests (cherchez le tag div avec l'id `\"naboo_artist_social_small\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request\n",
    "artist = 3037\n",
    "response = requests.get(f'https://www.deezer.com/fr/artist/{artist}')\n",
    "print(f\"{len(response.content):,.0f} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content)\n",
    "n = soup.find(\"div\", attrs={\"id\": \"naboo_artist_social_small\"}).find(\"span\").text\n",
    "print(f\"{int(n):,.0f} fans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le terme \"API\" est très générique et peut désigner bien des choses, mais dans le jargon on l'utilise souvent pour désigner un service web qui renvoie non pas:\n",
    "> des pages web au format HTML (destinées à être lues par un humain dans son navigateur)\n",
    "\n",
    "mais:\n",
    "> des données au format JSON (ou texte, ou XML, destinées à être traitées par un programme)\n",
    "\n",
    "![img](https://miro.medium.com/max/4238/1*OcmVkcsM5BWRHrg8GC17iw.png)\n",
    "\n",
    "Puisque les API sont dédiées à l'usage via des programmes, elles disposent en général d'une bonne documentation, et sont fiables et stables dans le temps. Tandis que sur des pages web HTML classiques, le design peut par exemple changer du jour au lendemain et rendre votre programme BeautifulSoup obsolète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API JSON\n",
    "response = requests.get(f'https://api.deezer.com/artist/{artist}')\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_fan\n",
    "print(f\"{data['nb_fan']:,.0f} fans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picture\n",
    "from IPython import display\n",
    "\n",
    "url = data['picture']\n",
    "r = requests.get(url)\n",
    "display.Image(data=r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracklist\n",
    "url = data[\"tracklist\"]\n",
    "r = requests.get(url)\n",
    "dico = r.json()\n",
    "dico[\"data\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "- Récupérer la liste des titres avec une list comprehension,\n",
    "- Jouer un morceau à partir du titre avec `IPython.display.Video()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avantages d'une API\n",
    "- renvoie du format JSON, (ou texte ou XML), facile et rapide à traiter\n",
    "- renvoie un format stable et documenté (voire versionné)\n",
    "- exemple : https://github.com/bluesky-social/atproto\n",
    "- la documentation indique comment interagir avec l'API:\n",
    "    - quelle URL utiliser,\n",
    "    - quelle méthode HTTP (GET, POST, ...),\n",
    "    - quels paramètres utiliser,\n",
    "    - ...\n",
    "\n",
    "=> idéal pour les développeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quel intérêt pour le fournisseur d'API ?\n",
    "\n",
    "En général il met en place des quotas de requêtes ou d'autres limitations afin de proposer un service payant qui dispose de possibilités avancées / d'un meilleur support / etc.\n",
    "\n",
    "C'est pourquoi de nombreux services nécessitent de se connecter avec son compte client pour utiliser une API (e.g. https://openweathermap.org/api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Auth\n",
    "\n",
    "Exemple: accéder à https://yotta-conseil.fr/private/\n",
    "\n",
    "Pour y accéder il est nécessaire d'utiliser les credentials suivant:\n",
    "- *login* : `'admin'`\n",
    "- *password* : `'secret'`\n",
    "\n",
    "Si on ne les passe pas (ou si on ne passe pas les bons), on a obtient une erreur 401 (= unauthorized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sans login/password\n",
    "res = requests.get('https://yotta-conseil.fr/private')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec login password\n",
    "res = requests.get('https://yotta-conseil.fr/private', auth=('admin', 'secret'))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contenu\n",
    "res.content.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;\">Attention, ne pas mettre les credentials dans le code !</p>\n",
    "\n",
    "Exemple avec la librairie **dotenv**.\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "La librairie peut gérer les variables d'environnement et également parser des fichiers de type `.env`.\n",
    "\n",
    "```txt\n",
    "username=admin\n",
    "password=secret\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env pour masquer auth credentials\n",
    "# fichier .env avec les credentials\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env0\")\n",
    "\n",
    "requests.get('https://yotta-conseil.fr/private', auth=(config['username'], config['password']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auth par token\n",
    "\n",
    "Exemple sur openweathermap :\n",
    "- documentation: https://openweathermap.org/appid\n",
    "- mes tokens: https://home.openweathermap.org/api_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requête avec un token\n",
    "config = dotenv_values(\".env\")\n",
    "token = config[\"owm_token\"]\n",
    "n = 4\n",
    "print(f\"{token[:n]}{\"x\"*(len(token)-2*n)}{token[-n:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantage des tokens :\n",
    "- évite que les requêtes HTTP contiennent le mot de passe - à la place elles contiennent un token\n",
    "- si je me fais \"voler\" un token, je peux le supprimer de mon compte et en créer un nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requête avec un token\n",
    "ville=\"Paris\"\n",
    "url = f\"http://api.openweathermap.org/data/2.5/weather?APPID={token}&q={ville}\"\n",
    "res = requests.get(url)\n",
    "meteo = res.json()\n",
    "meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type\n",
    "type(meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractions\n",
    "{'city': meteo['name'],\n",
    "'country': meteo['sys']['country'],\n",
    "'date': meteo['dt'],\n",
    "'temp': meteo['main']['temp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractions\n",
    "import time\n",
    "\n",
    "{'city': meteo['name'],\n",
    "'country': meteo['sys']['country'],\n",
    "'date': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(meteo['dt'])),\n",
    "'temp': f\"{meteo['main']['temp'] - 273.15:.1f}\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "Se créer un compte sur **BlueSky** et requêter des posts. Nécessite d'installer la librairie **atproto**.\n",
    "\n",
    "Doc : https://atproto.blue/en/latest/atproto/atproto_client.models.app.bsky.feed.search_posts.html\n",
    "\n",
    "```python\n",
    "import atproto\n",
    "\n",
    "# get credentials\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# atproto client\n",
    "client = atproto.Client()\n",
    "client.login(config['username_bsky'], config['password_bsky'])\n",
    "\n",
    "# réponse limitée à 25 posts par défaut, maximum = 100, ensuite il faut gérer un curseur\n",
    "response = client.app.bsky.feed.search_posts(q=\"...\", ...)\n",
    "```\n",
    "\n",
    "Liste des posts : `response.posts`\n",
    "\n",
    "Pour chaque post :\n",
    "- `post.record` (`.text`, `.created_at`)\n",
    "- `post.author` (`.handle`)\n",
    "- `post.uri`\n",
    "- `post.*_count` (engagements : bookmark, like, quote, reply, repost)\n",
    "\n",
    "Ensuite, possibilité de faire du NLP sur les textes des posts.\n",
    "\n",
    "```python\n",
    "# réponse limitée également, ensuite il faut gérer un curseur\n",
    "response = client.app.bsky.graph.get_followers(actor=\"...\", ...)\n",
    "```\n",
    "\n",
    "Ensuite, possibilité de faire des graphes sur les followers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
